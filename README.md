
# ğŸ§  Deducao-Natural-com-LLMs

RepositÃ³rio auxiliar da dissertaÃ§Ã£o:

> **AVALIANDO O DESEMPENHO DE GRANDES MODELOS DE LINGUAGEM NA REALIZAÃ‡ÃƒO DE PROVAS DE DEDUÃ‡ÃƒO NATURAL EM LÃ“GICA PROPOSICIONAL E LÃ“GICA DE PREDICADOS**  

## ğŸ“˜ Resumo

A crescente utilizaÃ§Ã£o de agentes conversacionais e de Grandes Modelos de Linguagem (LLMs) tem impulsionado investigaÃ§Ãµes sobre seu potencial em tarefas de raciocÃ­nio lÃ³gico.  
Este trabalho avalia o desempenho de modelos de linguagem de grande porte â€” **GPT-4.1-mini**, **GPT-4o** e **GPT-3.5-turbo** â€” na resoluÃ§Ã£o de **provas de deduÃ§Ã£o natural** em **LÃ³gica Proposicional** e **LÃ³gica de Predicados**.

Foram elaboradas **bases de dados prÃ³prias**, com enunciados e provas completas no formato utilizado pela ferramenta **NADIA (Natural Deduction Assistant)**.  
Os modelos foram testados **sem treinamento prÃ©vio** e, posteriormente, foi realizado **fine-tuning no GPT-4.1-mini** com exemplos anotados.  

Os resultados indicam melhora significativa apÃ³s o treinamento, mas tambÃ©m evidenciam limitaÃ§Ãµes dos modelos no raciocÃ­nio formal simbÃ³lico.

---

## ğŸ§© Objetivos

### ğŸ¯ Objetivo geral
Avaliar o desempenho de Grandes Modelos de Linguagem (LLMs) na realizaÃ§Ã£o de provas de deduÃ§Ã£o natural em lÃ³gica proposicional e de predicados.

### ğŸ”¹ Objetivos especÃ­ficos
- Construir bases de dados de exercÃ­cios e soluÃ§Ãµes no formato NADIA;  
- Avaliar e comparar o desempenho dos modelos **GPT-4.1-mini**, **GPT-4o** e **GPT-3.5-turbo** sem treinamento;  
- Aplicar fine-tuning no **GPT-4.1-mini** e comparar resultados antes e depois do treinamento;  
- Analisar a precisÃ£o e os tipos de erro apresentados nas respostas geradas.

---

## ğŸ“‚ Datasets

Os datasets foram elaborados manualmente a partir de exercÃ­cios de **deduÃ§Ã£o natural** em formato textual.  
Cada instÃ¢ncia contÃ©m um **enunciado** e uma **prova-resposta esperada**, no padrÃ£o da ferramenta **NADIA**.  
Os arquivos estÃ£o disponÃ­veis em formato `.csv`.

### ğŸ”¹ Dataset de Treinamento
- **Arquivo:** [ğŸ“„ Dataset â€” DeduÃ§Ã£o Natural â€” Treinamento](https://github.com/leonardomartins777/Deducao-Natural-com-LLMs/blob/main/Dataset%20-%20Dedu%C3%A7%C3%A3o%20Natural%20-%20Dataset%20Treinamento.csv)
- **Subconjuntos:**
  | Subconjunto | Tipo de LÃ³gica | Linhas | DescriÃ§Ã£o |
  |--------------|----------------|--------|------------|
  | **BD2** | Proposicional | 102â€“201 | ExercÃ­cios proposicionais utilizados no treinamento |
  | **BD4** | Predicados | 2â€“101 | ExercÃ­cios de lÃ³gica de predicados utilizados no treinamento |

### ğŸ”¹ Dataset de Teste
- **Arquivo:** [ğŸ“„ Dataset â€” DeduÃ§Ã£o Natural â€” Testes](https://github.com/leonardomartins777/Deducao-Natural-com-LLMs/blob/main/Dataset%20-%20Dedu%C3%A7%C3%A3o%20Natural%20-%20Dataset%20Testes.csv)
- **Subconjuntos:**
  | Subconjunto | Tipo de LÃ³gica | Linhas | DescriÃ§Ã£o |
  |--------------|----------------|--------|------------|
  | **BD1** | Proposicional | 22â€“62 | ExercÃ­cios proposicionais utilizados para teste |
  | **BD3** | Predicados | 2â€“21 | ExercÃ­cios de lÃ³gica de predicados utilizados para teste |


## ğŸ“Š Resultados

A seguir sÃ£o apresentados os principais resultados obtidos na avaliaÃ§Ã£o dos modelos de linguagem na resoluÃ§Ã£o de provas de **deduÃ§Ã£o natural** em **LÃ³gica Proposicional** e **LÃ³gica de Predicados**.

### ğŸ”¹ Desempenho geral por modelo e base

O grÃ¡fico abaixo ilustra a taxa de acertos de cada modelo nos conjuntos de dados de teste (BD1 e BD3):

<p align="center">
  <img src="resultados/graficos/Resultados-Grafico-simplificado.png" alt="GrÃ¡fico de desempenho dos modelos" width="700">
</p>


**Legenda:**
- **GPT-4.1-mini (sem FT)** â€” versÃ£o original do modelo **GPT-4.1-mini**, utilizada **sem fine-tuning** na fase inicial dos experimentos. Serviu como base de comparaÃ§Ã£o direta para avaliar o impacto do ajuste supervisionado.  
- **GPT-4.1-mini (com FT)** â€” versÃ£o do **GPT-4.1-mini** **ajustada via fine-tuning supervisionado** com os exemplos do *Dataset de Treinamento* (BD2 e BD4). Empregada na etapa final de avaliaÃ§Ã£o, demonstrou **maior coerÃªncia formal e precisÃ£o** nas provas de deduÃ§Ã£o natural.  
- **GPT-4o** â€” modelo testado **sem fine-tuning**.  
- **GPT-3.5-turbo** â€” modelo **base de comparaÃ§Ã£o**, tambÃ©m **sem fine-tuning**.
  

O **GPT-4.1-mini treinado** apresentou melhora significativa, sobretudo nas provas de lÃ³gica de predicados, demonstrando maior estabilidade nas regras de inferÃªncia e consistÃªncia nas conclusÃµes.  
Os modelos nÃ£o ajustados obtiveram desempenho satisfatÃ³rio apenas em exercÃ­cios mais diretos, falhando nas provas que exigem subprovas ou raciocÃ­nio de segunda ordem.

---